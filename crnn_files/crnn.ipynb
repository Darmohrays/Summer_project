{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"crnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["cpYQUQhgN3lh","lPsjAhjFMEKz","OxEfGWYbHss6","dJyPIjcbOzhV","h012Mpp92tjC","styq9DzJ22Fi","G_oh5L0_aWNK","aUSW4HLf_1uO","hcQHB5WtlUUB","BbfJMeZ6Sk3V","3ldO3rwVSpeW","SWmAsQ5IWzjN","5xLGXzyAJTkH","OqUT-NA178g6","fwJs6JLE7qp_"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cpYQUQhgN3lh","colab_type":"text"},"source":["# import"]},{"cell_type":"code","metadata":{"id":"Ie2O1BGyJQd1","colab_type":"code","colab":{}},"source":["#! pip freeze\n","#! pip install keras==2.2.4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqIvstxcN5x3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYgoAv4MN5Cw","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","from copy import deepcopy\n","import string\n","from PIL import Image, ImageDraw, ImageFont"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPsjAhjFMEKz","colab_type":"text"},"source":["# parameters"]},{"cell_type":"code","metadata":{"id":"Yu4Dtck1MOAU","colab_type":"code","colab":{}},"source":["CHAR_VECTOR = string.ascii_letters + string.digits + '.'\n","letters = [letter for letter in CHAR_VECTOR]\n","\n","num_classes = len(letters) + 1\n","\n","img_w, img_h = 128, 32\n","\n","# Network parameters\n","batch_size = 64\n","val_batch_size = 10\n","\n","downsample_factor = 4\n","max_text_len = 16"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxEfGWYbHss6","colab_type":"text"},"source":["# model"]},{"cell_type":"code","metadata":{"id":"QwuKscZIOwR0","colab_type":"code","outputId":"9d75e787-3c28-4f48-bc9a-efdaa3d759da","executionInfo":{"status":"ok","timestamp":1567662533897,"user_tz":-180,"elapsed":1923,"user":{"displayName":"yura it's raining today","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhDS418TZysN6H8syy27wT4d5PQoynXsSD92Xk=s64","userId":"15239198560947031173"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["from keras import backend as K\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Input, Dense, Activation\n","from keras.layers import Reshape, Lambda, BatchNormalization\n","from keras.layers.merge import add, concatenate\n","from keras.models import Model\n","from keras.layers.recurrent import LSTM\n","K.set_learning_phase(0) # keras test mode\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","WARNING: Logging before flag parsing goes to stderr.\n","W0905 05:48:52.591287 140364199167872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"X4Oclg75HrwJ","colab_type":"code","colab":{}},"source":["# labels (samples, max_string_length)\n","# y_pred (samples, time_steps, num_categories)\n","# input_length (samples, 1) - y_pred sequences lens\n","# label_length (samples, 1) - y_true sequences lens\n","\n","# # Loss and train functions, network architecture\n","def ctc_lambda_func(args):                                                                     # !!!___use *args to get all agrs___\n","    y_pred, labels, input_length, label_length = args\n","    # the 2 is critical here since the first couple outputs of the RNN\n","    # tend to be garbage:\n","    y_pred = y_pred[:, 2:, :]                                                                  # !!!___pass first 2 steps out___\n","    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n","\n","  \n","  \n","# auto strides 2x2 in max_pool\n","# last padding same  [was 0]\n","# different input_size  [was img_w x 32]\n","# more batch_norm layers  [was 2]\n","# different CNN to RNN connector  [was Map-To-Sequence]\n","# auto flatten in dense\n","# auto batch dim in the layers\n","def get_Model(training):\n","    input_shape = (img_w, img_h, 1)     # (128, 32, 1) grayscale image\n","\n","    # Make Network\n","    inputs = Input(name='the_input', shape=input_shape, dtype='float32')  # (None, 128, 32, 1)\n","\n","    # Convolution layer (VGG)\n","    inner = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)  # (None, 128, 32, 64)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)  # (None, 64, 16, 64)\n","\n","    inner = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  # (None, 64, 16, 128)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)  # (None, 32, 8, 128)\n","\n","    inner = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)  # (None, 32, 8, 256)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(inner)  # (None, 32, 8, 256)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)  # (None, 32, 4, 256)\n","\n","    inner = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(inner)  # (None, 32, 4, 512)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = Conv2D(512, (3, 3), padding='same', name='conv6')(inner)  # (None, 32, 4, 512)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","    inner = MaxPooling2D(pool_size=(1, 2), name='max4')(inner)  # (None, 32, 2, 512)\n","\n","    inner = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(inner)  # (None, 32, 2, 512)\n","    inner = BatchNormalization()(inner)\n","    inner = Activation('relu')(inner)\n","\n","    \n","    # CNN to RNN\n","    inner = Reshape(target_shape=((32, 1024)), name='reshape')(inner)  # (None, 32, 1024)\n","    inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 32, 64)\n","\n","    \n","    # RNN layer\n","    lstm_1 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(inner)  # (None, 32, 512)\n","    lstm_1b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(inner)\n","    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n","\n","    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n","    lstm1_merged = BatchNormalization()(lstm1_merged)\n","    \n","    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n","    lstm_2b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n","    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n","\n","    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n","    lstm2_merged = BatchNormalization()(lstm2_merged)\n","\n","    # transforms RNN output to character activations:\n","    inner = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(lstm2_merged) #(None, 32, 63)\n","    y_pred = Activation('softmax', name='softmax')(inner)\n","\n","    labels = Input(name='the_labels', shape=[max_text_len], dtype='float32') # (None ,8)\n","    input_length = Input(name='input_length', shape=[1], dtype='int64')     # (None, 1)\n","    label_length = Input(name='label_length', shape=[1], dtype='int64')     # (None, 1)\n","\n","    # Keras doesn't currently support loss funcs with extra parameters\n","    # so CTC loss is implemented in a lambda layer\n","    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n","\n","    if training:\n","        return Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n","    else:\n","        return Model(inputs=[inputs], outputs=y_pred)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJyPIjcbOzhV","colab_type":"text"},"source":["# input data"]},{"cell_type":"markdown","metadata":{"id":"h012Mpp92tjC","colab_type":"text"},"source":["## font paths"]},{"cell_type":"code","metadata":{"id":"Qh_evxsI2_mo","colab_type":"code","outputId":"cb215afa-a8c4-4904-8745-91c61fc79055","executionInfo":{"status":"ok","timestamp":1567662581853,"user_tz":-180,"elapsed":41373,"user":{"displayName":"yura it's raining today","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhDS418TZysN6H8syy27wT4d5PQoynXsSD92Xk=s64","userId":"15239198560947031173"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# add calibri later\n","font_paths = {\n","    'roboto_mono': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/roboto-mono/RobotoMono-Regular.ttf',\n","    'arial': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/Arial/ArialRegular/ArialRegular.ttf',\n","    'armino': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/arimo/Arimo-Regular.ttf',\n","    'helvetica': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/helvetica/HelveticaRegular/HelveticaRegular.ttf',\n","    'open_sans': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/open-sans/OpenSans-Regular.ttf',\n","    'roboto2014': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/roboto-2014/Roboto-Regular.ttf',\n","    'times_new_roman': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/times-new-roman/times-new-roman.ttf',\n","    'calibri': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/calibri font sv/Calibri Regular/Calibri Regular.ttf',\n","    'courier': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/COURIER/COURIER.TTF',\n","    'lato': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/Lato2OFL/Lato2OFL/Lato-Regular.ttf',\n","    'shadows_into_light': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/Shadows-into-light_Typeface_1813_(Fontmirror)/Shadows Into Light 400.ttf',\n","    'abel': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/abel/abel-regular.ttf',\n","    'abril_fatface': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/abril-fatface/AbrilFatface-Regular.otf',\n","    'acme': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/acme/Acme-Regular.ttf',\n","    'amatic': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/amatic/Amatic-Bold.ttf',\n","    'anton': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/anton/Anton.ttf',\n","    'archivo': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/archivo/Archivo-Regular.ttf',\n","    'arimo': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/arimo/Arimo-Regular.ttf',\n","    'asap': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/asap_regular/asap.regular.ttf',\n","    'assistant': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/assistant/Assistant-Regular.otf',\n","    'barlow': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/barlow_regular/barlow.regular.ttf',\n","    'bitter': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/bitter/Bitter-Regular.ttf',\n","    'bree_serif': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/bree_serif/bree-serif.regular.ttf',\n","    'cabin': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/cabin/cabin.regular.ttf',\n","    'cairo': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/cairo/Cairo Medium.ttf',\n","    'catamaran': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/catamaran/Catamaran-Regular.ttf',\n","    'caveat': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/caveat/caveat-regular.ttf',\n","    'comfortaa': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/comfortaa/Comfortaa-Regular.ttf',\n","    'comic_sans': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/comic-sans-ms/COMIC.TTF',\n","    'crimson_text': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/crimson-text/CrimsonText-Roman.ttf',\n","    'dancing': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/dancing/dancing-script.regular.ttf',\n","    'dosis': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/dosis/dosis.book.ttf',\n","    'exo': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/exo/Exo-Regular.ttf',\n","    'firasans': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/firasans/FiraSans-Regular.otf',\n","    'fjalla_one': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/fjalla-one/FjallaOne-Regular.ttf',\n","    'garamond': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/garamond/Garamond.ttf',\n","    'georgia': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/georgia-2-cufonfonts/georgia.ttf',\n","    'hind': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/hind/Hind-Regular.ttf',\n","    'impact': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/impact/impact.ttf',\n","    'inconsolata': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/inconsolata/Inconsolata-Regular.ttf',\n","    'indie_flower': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/indie_flower_16818/IndieFlower.ttf',\n","    'josefin_sans': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/josefin-sans/JosefinSans-Regular.ttf',\n","    'kanit': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/kanit/Kanit-Regular.ttf',\n","    'karla': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/karla/Karla-Regular.ttf',\n","    'lobster': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/lobster/The-Lobster-Font/fonts/TTF/Lobster.ttf',\n","    'open_sans': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/open-sans/OpenSans-Regular.ttf',\n","    'oswald': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/oswald/Oswald-Regular.ttf',\n","    'pacifico': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/pacifico/Pacifico.ttf',\n","    'quicksand': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/quicksand/Quicksand-Regular.ttf',\n","    'raleway': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/raleway/Raleway-Regular.ttf',\n","    'righteous': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/righteous/Righteous-Regular.ttf',\n","    'roboto': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/roboto/Roboto-Regular.ttf',\n","    'ubuntu': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/ubuntu/ubuntu-font-family-0.80/Ubuntu-R.ttf',\n","    'yanone': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/fonts/yanone-kaffeesatz/YanoneKaffeesatz-Regular.ttf'\n","    \n","}\n","\n","\n","tmp_path = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x'\n","late_paths = []\n","for path in os.listdir(tmp_path):\n","  if not len(os.listdir(os.path.join(tmp_path,path))):\n","    late_paths.append(path)\n"," \n","print(late_paths)\n","print(len(late_paths))\n","\n","font_names = [i for i in font_paths.keys() if i not in late_paths]\n","print(len(font_names))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[]\n","0\n","53\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"styq9DzJ22Fi","colab_type":"text"},"source":["## dataset functions"]},{"cell_type":"code","metadata":{"id":"-g5zexVXO3J1","colab_type":"code","colab":{}},"source":["# data_dir_path - шлях до директорії із трейн сетом (шрифтами)\n","# font_names [fonts_n] - масив назв шрифтів\n","# curent_batch_size - кількість випадкових слів\n","# font_p [len(font_names)] - ймовірності для кожного шрифту попасти в датасет\n","#\n","# rand_word_paths [batch_size] - випадкові адреси зображеннь слів\n","def get_random_word_paths(data_dir_path, font_names, current_batch_size, font_p=None):\n","  rand_font_name = np.random.choice(font_names, p=font_p) # 1 font choice\n","  font_dir_path = os.path.join(data_dir_path, rand_font_name)\n","  \n","  rand_img_name = np.random.choice(os.listdir(font_dir_path)) # 1 img choice\n","  img_dir_path = os.path.join(font_dir_path, rand_img_name)\n","  \n","  word_names = os.listdir(img_dir_path)\n","  batch_chunk = len(word_names) - current_batch_size # batch_words \n","  if batch_chunk >= 0: # вистачає слів для батча в цьому зображенні\n","    rand_words = np.random.choice(word_names, current_batch_size)\n","    rand_word_paths = [os.path.join(img_dir_path, w_n) for w_n in rand_words]\n","  else: # для батча треба більше слів ніж є в даному зображенні\n","    rand_words = np.random.choice(word_names, len(word_names))\n","    rand_word_paths = [os.path.join(img_dir_path, w_n) for w_n in rand_words]\n","    rand_word_paths += get_random_word_paths(data_dir_path, font_names, abs(batch_chunk), font_p) # добираю слова до batch_size    \n","    \n","  return rand_word_paths\n","\n","\n","# data_dir_path - шлях до директорії із трейн сетом (шрифтами)\n","# font_names [fonts_n] - масив назв шрифтів\n","# batch_size - кількість зображеннь слів\n","# max_word_len - максимальна довжина слова\n","# font_p [len(font_names)] - ймовірності для кожного шрифту попасти в датасет\n","# downsample_factor - наскільки пулінги зменшують зображення \n","#\n","# batch_x [batch_size, 128, 32] - зображення слів\n","# batch_y [batch_size, max_word_len] - індекси літер для слів + нулі(якщо word_len<max_word_len)\n","# input_lens [batch_size, 1] - розмір виходів з рнн із врахуванням пулінгів\n","# word_lens [batch_size, 1] - довжини слів\n","def get_batch(data_dir_path, font_names, batch_size, max_word_len, font_p=None, downsample_factor=4):\n","  batch_x = np.zeros((batch_size, 128, 32))\n","  batch_y = np.zeros((batch_size, max_word_len))\n","  word_lens = np.zeros((batch_size, 1)) \n","  input_lens = np.ones((batch_size, 1)) * (128 // downsample_factor - 2)\n","  rand_words_adress = get_random_word_paths(data_dir_path, font_names, batch_size, font_p) # batch випадкових адес слів\n","            \n","  for i, word_path in enumerate(rand_words_adress):\n","    batch_x[i] = minmax_image(cv2.imread(word_path, 0)).T\n","    word_name = word_path.split('/')[-1][:-4] # без .jpg\n","    char_indexes = list(map(int, word_name.split('_')))\n","                  \n","    if len(char_indexes) > max_word_len: # обрізаю задовге слово\n","      char_indexes = char_indexes[:max_word_len]\n","    ch_len = len(char_indexes) \n","    word_lens[i][0] = ch_len\n","    batch_y[i][:ch_len] = char_indexes\n","  batch_x = np.reshape(batch_x, [-1,128,32,1])\n","  return batch_x, batch_y, input_lens, word_lens\n","\n","\n","# image [h, w] - зображення\n","# mm_image [h,w] - мін-макс нормалізоване зображення\n","def minmax_image(image):\n","  mm_image = image-image.min() + 0.001\n","  mm_image = mm_image/mm_image.max() - 0.0001\n","  return mm_image\n","\n","\n","# images [batch, h, w] - зображення\n","# st_images [batch, h, w] - стандартизовані по батчу зображення\n","def standartize_images(images):\n","  st_images = (images - np.mean(images))\n","  st_images /= (np.std(st_images))\n","  return st_images\n","\n","    \n","    # генерує дані для входу і виходу молелі\n","def data_gen2(font_names, batch_size, max_word_len, font_p, data_dir_path, train=1):\n","  while True:\n","    x_batch, y_batch, input_lens, word_lens = get_batch(data_dir_path, font_names, batch_size, max_word_len, font_p)\n","    inputs = {\n","        'the_input': x_batch, \n","        'the_labels': y_batch,\n","        'input_length': input_lens,\n","        'label_length': word_lens\n","    }\n","    outputs = {'ctc': np.zeros([batch_size])} # не юзається бо лосс функція не юзає output, а юзає лейбли з інпута \n","    yield (inputs, outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUEbM7f9SlUO","colab_type":"code","colab":{}},"source":["for sample in range(10):\n","  x,y = list(data_gen())[0]\n","  print('{}\\n\\n{}\\n\\n{}'.format(x,y,l))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgtNSx9eO56f","colab_type":"code","colab":{}},"source":["\n","data_dir_path = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x'\n","font_names = ['roboto_mono', 'helvetica', 'comic_sans', 'arial', 'times_new_roman']\n","batch_size = 10\n","max_word_len = 16\n","font_p = [0.3, 0.2, 0.1, 0.2, 0.2] \n","\n","#rnd_w_pths = get_random_word_paths(data_dir_path, font_names, batch_size, font_p)\n","batch_x, batch_y, input_lens, word_lens = get_batch(data_dir_path, font_names, batch_size, max_word_len, font_p)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1HXxXdrO_B3","colab_type":"code","colab":{}},"source":["# len([pth for pth in rnd_w_pths if 'roboto_mono' in pth])    # font words num\n","print(batch_x.shape, batch_y.shape, input_lens.shape, word_lens.shape, sep='\\n')\n","\n","\n","'''\n","print(len(batch_x), batch_x[1], sep='\\n')\n","print('',len(batch_y), batch_y[1], sep='\\n')\n","print('\\n',word_lens[1])\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_oh5L0_aWNK","colab_type":"text"},"source":["## random char batches"]},{"cell_type":"code","metadata":{"id":"zbiN92JUaa-n","colab_type":"code","colab":{}},"source":["# # batch_size - кількість зображеннь слів\n","# font_name - наза шрифта\n","# max_word_len - максимальна довжина слова \n","# alphabet_arr - масив з алфавітом\n","# font_paths - словник {назва шрифа: адрес шрифта}\n","# downsample_factor - наскільки пулінги зменшують зображення вширину\n","def data_gen_rand(batch_size, font_name, max_word_len, alphabet_arr, font_paths, downsample_factor=4):\n","  while 1:\n","    batch_x = np.zeros((batch_size, 128, 32))\n","    batch_y = np.zeros((batch_size, max_word_len))\n","    word_lens = np.zeros((batch_size, 1)) \n","    input_lens = np.ones((batch_size, 1)) * (128 // downsample_factor - 2)\n","    \n","    img = Image.new('L', (128, 32), color = 255)\n","    font = ImageFont.truetype(font_paths[font_name], 14)\n","    \n","    for i in range(batch_size):\n","      rand_len = np.random.choice(max_word_len-1)+1\n","      shuffled_alphabet = deepcopy(alphabet_arr)\n","      np.random.shuffle(shuffled_alphabet)\n","      word =  ''.join(np.random.choice(shuffled_alphabet, rand_len))\n","      word_indexes = indexes_from_word(word, ''.join(alphabet_arr))\n","      \n","      batch_y[i, :rand_len] = (word_indexes)\n","      word_lens[i, 0] = rand_len\n","      \n","      temp_img = (img.copy())\n","      d = ImageDraw.Draw(temp_img)\n","      d.text((2, 12), word, font=font, fill=0)\n","      open_cv_image = np.array(temp_img).T\n","      open_cv_image = cv2.bilateralFilter(open_cv_image,9,100,100) #diameter, \n","      batch_x[i] = minmax_image(open_cv_image)\n","    batch_x = np.reshape(batch_x, [-1,128,32,1])\n","    inputs = {\n","      'the_input': batch_x, \n","      'the_labels': batch_y,\n","      'input_length': input_lens,\n","      'label_length': word_lens\n","    }\n","    outputs = {'ctc': np.zeros([batch_size])} # не юзається бо лосс функція не юзає output, а юзає лейбли з інпута \n","    yield inputs, outputs\n","     \n","      \n","# word - стрічка\n","# alphabet_str - str алфавіт символів\n","# indexes - arr, заміна символів у слові на їх індекси в алфавіті      \n","def indexes_from_word(word, alphabet_str):\n","  indexes = []\n","  for char in word:\n","    indexes.append(alphabet_str.find(char))\n","  return indexes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UR_sKNs5chHl","colab_type":"code","outputId":"f28c4bdd-ff36-4d31-9a40-c109dab21c18","executionInfo":{"status":"ok","timestamp":1567626689206,"user_tz":-180,"elapsed":592,"user":{"displayName":"yura it's raining today","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhDS418TZysN6H8syy27wT4d5PQoynXsSD92Xk=s64","userId":"15239198560947031173"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["s1 = '1 2 3 4 5'.split(' ')\n","s2 = deepcopy(s1)\n","np.random.shuffle(s2)\n","print(s2)\n","print(s1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['3', '5', '2', '4', '1']\n","['1', '2', '3', '4', '5']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aUSW4HLf_1uO","colab_type":"text"},"source":["# training"]},{"cell_type":"code","metadata":{"id":"fSUtHH5h0VDE","colab_type":"code","colab":{}},"source":["from keras.optimizers import Adadelta, Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","K.set_learning_phase(0)\n","\n","\n","model = get_Model(training=True)\n","weights_path = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/fonts_53_images_40/LSTM+BN5--100--1.284.hdf5'\n","weights_path2 = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new/LSTM+BN5--100--0.181.hdf5'\n","\n","try:\n","    model.load_weights(weights_path2)\n","    print(\"...Previous weight data...\")\n","except:\n","    print(\"...New weight data...\")\n","\n","\n","train_dir_path = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x'\n","train_dir_path2 = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x_new'\n","train_dir_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x_new'\n","\n","test_dir_path = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test/x'\n","test_dir_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test/x_new'\n","\n","\n","data_gen_kwargs_train = {\n","  'font_names' : ['roboto_mono'], #[i for i in font_paths.keys() if i not in late_paths],\n","  'batch_size' : 64,\n","  'max_word_len' : 16,\n","  'font_p' : None,\n","  'data_dir_path' : train_dir_path3\n","}\n","data_gen_kwargs_test = {\n","  'font_names' : ['roboto_mono'], #[i for i in font_paths.keys() if i not in late_paths],\n","  'batch_size' : 64,\n","  'max_word_len' : 16,\n","  'font_p' : None,\n","  'data_dir_path' : test_dir_path3\n","}\n","\n","\n","\n","#adam = Adam()\n","ada = Adadelta()\n","\n","check_path = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new'\n","early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n","checkpoint = ModelCheckpoint(filepath=os.path.join(check_path, 'LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5'), monitor='loss', verbose=1, mode='min', period=1)\n","# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n","model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n","\n","# captures output of softmax so we can decode the output during visualization\n","model.fit_generator(generator = data_gen2(**data_gen_kwargs_train),\n","                    steps_per_epoch = 100,\n","                    epochs = 100,\n","                    callbacks = [checkpoint],\n","                    validation_data = data_gen2(**data_gen_kwargs_test),\n","                    validation_steps = 5\n","                    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcQHB5WtlUUB","colab_type":"text"},"source":["# training rand"]},{"cell_type":"code","metadata":{"id":"hIOjgz_5lWXo","colab_type":"code","colab":{}},"source":["from keras.optimizers import Adadelta, Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","K.set_learning_phase(0)\n","\n","\n","model = get_Model(training=True)\n","weights_path = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/fonts_53_images_40/LSTM+BN5--100--1.284.hdf5'\n","weights_path2 = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new/LSTM+BN5--100--0.181.hdf5'\n","weights_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new/LSTM+BN5--38--0.471.hdf5'\n","\n","try:\n","    model.load_weights(weights_path3)\n","    print(\"...Previous weight data...\")\n","except:\n","    print(\"...New weight data...\")\n","\n","data_gen_kwargs = {\n","   'batch_size' : 80,\n","  'font_name' : 'roboto_mono',\n","  'max_word_len' : 16,\n","  'alphabet_arr': list(string.ascii_letters + string.digits + '.'),\n","  'font_paths': font_paths\n","}\n","\n","#adam = Adam()\n","ada = Adadelta()\n","\n","check_path = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new'\n","early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n","checkpoint = ModelCheckpoint(filepath=os.path.join(check_path, 'LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5'), monitor='loss', verbose=1, mode='min', period=1)\n","# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n","model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n","\n","# captures output of softmax so we can decode the output during visualization\n","model.fit_generator(generator = data_gen_rand(**data_gen_kwargs),\n","                    steps_per_epoch = 200,\n","                    epochs = 100,\n","                    callbacks = [checkpoint],\n","                    validation_data = data_gen_rand(**data_gen_kwargs),\n","                    validation_steps = 5\n","                    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nytvZjXVchT0","colab_type":"text"},"source":["# predict"]},{"cell_type":"markdown","metadata":{"id":"uaDJzQ5rSf9p","colab_type":"text"},"source":["## load model and weights"]},{"cell_type":"code","metadata":{"id":"OzOTlfxn0XT8","colab_type":"code","outputId":"17051287-a3ac-4d67-91bd-314ca2a02fb3","executionInfo":{"status":"ok","timestamp":1567679023138,"user_tz":-180,"elapsed":5250,"user":{"displayName":"yura it's raining today","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhDS418TZysN6H8syy27wT4d5PQoynXsSD92Xk=s64","userId":"15239198560947031173"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["K.set_learning_phase(0)\n","\n","model = get_Model(training=False)\n","weights_path = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_images40/LSTM+BN5--100--0.001.hdf5'\n","weights_path2 = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new/LSTM+BN5--100--0.181.hdf5'\n","weights_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/crnn_checkpoints/roboto_mono_images_40_new/LSTM+BN5--38--0.471.hdf5'\n","\n","try:\n","    model.load_weights(weights_path3)\n","    print(\"...Previous weight data...\")\n","except:\n","    raise Exception(\"No weight file!\")\n","    \n"],"execution_count":122,"outputs":[{"output_type":"stream","text":["...Previous weight data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BbfJMeZ6Sk3V","colab_type":"text"},"source":["## decode funcs"]},{"cell_type":"code","metadata":{"id":"X5kPwmZA0cL1","colab_type":"code","colab":{}},"source":["import itertools\n","\n","'''\n","# y_pred [32, num_classes] - Розподіл ймовірностей для 32 ділянок фото\n","# highest_prob_chars [word_len] - передбачене слово\n","def decode_label(y_pred):\n","    # out : (1, 32, 64)\n","    highest_prob_classes = list(np.argmax(y_pred[2:], axis=1)) # індекс класу з макс ймовірністю\n","    highest_prob_classes = [k for k, g in itertools.groupby(highest_prob_classes)]  # забирає суміжні повторні значення\n","    highest_prob_chars = get_chars_from_indexes(highest_prob_classes, letters) # видаляє пропуски, декодує індекси\n","    return highest_prob_chars\n","'''\n","# y_pred [32, num_classes] - Розподіл ймовірностей для 32 ділянок фото\n","# highest_prob_chars [word_len] - передбачене слово\n","def decode_label(y_pred, alphabet_str):\n","    # out : (1, 32, 64)\n","    highest_prob_classes = list(np.argmax(y_pred[2:], axis=1)) # індекс класу з макс ймовірністю\n","    highest_prob_classes = [k for k, g in itertools.groupby(highest_prob_classes)]  # забирає суміжні повторні значення\n","    highest_prob_chars = get_chars_from_indexes(highest_prob_classes, alphabet_str) # видаляє пропуски, декодує індекси\n","    return highest_prob_chars  \n","  \n","  \n","# letters [class_num] - alphabet array\n","# chars - decoded chars str\n","def get_chars_from_indexes(indexes, alphabet_str):\n","  chars = ''.join([alphabet_str[int(i)] for i in indexes if i<len(alphabet_str)])\n","  return chars\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ldO3rwVSpeW","colab_type":"text"},"source":["## predict, accuraccy funcs"]},{"cell_type":"code","metadata":{"id":"ZhLTXdY70gvd","colab_type":"code","colab":{}},"source":["# y_pred [batch, 32, num_classes] - батч розподілів ймовірностей \n","# y_true [batch, max_word_len, 1] - батч масивів правильних індексів\n","# word_lens [batch, 1] - батч  правильних довжин правмльних слів та правильних індексів\n","# alphabet_str [num_classes] - алфавіт, символи всіх класів (lower+upper+digits+.)\n","#\n","# pred_words [batch_size, ?] - батч передбачених слів\n","# true_words [batch_size, words_len] - батч правильних слів\n","def get_words_from_y(y_pred, y_true, word_lens, alphabet_str):\n","  pred_words, true_words = [], []\n","  for i in range(len(y_true)):\n","    pred_word, true_word = get_word_form_y(y_pred[i], y_true[i, :int(word_lens[i,0])], alphabet_str)\n","    pred_words.append(pred_word)\n","    true_words.append(true_word)\n","  return pred_words, true_words\n","\n","\n","# pred_probs [32, num_classes] - розподіл ймовірностей для 32 ділянок зображеня\n","# true_indexes [word_lens[i], 1] - масив правильних індексів символів для даного слова\n","# alphabet_str 'num_classes' - алфавіт, символи всіх класів (lower-upper-digits-.)\n","#\n","# pred_ord - передбачене слово\n","# true_word - правильне слово\n","def get_word_form_y(pred_probs, true_indexes, alphabet_str):\n","  pred_word = decode_label(pred_probs, alphabet_str)\n","  true_word = get_chars_from_indexes(true_indexes, alphabet_str)\n","  return pred_word, true_word\n","\n","\n","# all_pred_words [num_samples, ?] - сампли з передбаченими словами\n","# all_true_words [num_samples, word_lens] - сампли з правдивими словами\n","#\n","# (good_preds/all_preds) - точність по словах\n","# bad_preds [?] - масив погано передбачених слів \n","def get_word_accuraccy(all_pred_words, all_true_words):\n","  good_preds = 0\n","  bad_preds = []\n","  all_preds = len(all_true_words)\n","  for pred_word, true_word in zip(all_pred_words, all_true_words):  \n","    if pred_word == true_word:\n","      good_preds +=1\n","    else:\n","      bad_preds.append(pred_word)\n"," \n","  return (good_preds/all_preds), bad_preds\n","  \n","\n","# pred_word - передбачене слово\n","# true_word - правильне слово\n","#\n","# good_chars_num - кількість правильно преедбачених символів по порядку\n","def get_char_good_preds(pred_word, true_word):\n","  good_chars_num = 0\n","  bad_chars = []\n","  for pred_ch, true_ch in zip(pred_word, true_word):\n","    if pred_ch == true_ch:\n","      good_chars_num += 1\n","    else: \n","      bad_chars.append(pred_ch)\n","  return good_chars_num, bad_chars\n"," \n","\n","# all_pred_words [num_samples, ?] - сампли з передбаченими словами\n","# all_true_words [num_samples, word_lens] - сампли з правдивими словами\n","#\n","# (good_preds_num / all_preds_num) - точність по символах\n","def get_char_accuraccy(all_pred_words, all_true_words):\n","  good_preds_num = 0\n","  all_preds_num = 0\n","  bad_preds = []\n","  for pred_word, true_word in zip(all_pred_words, all_true_words):\n","    good_chars_num, bad_chars = get_char_good_preds(pred_word, true_word)\n","    good_preds_num += good_chars_num\n","    all_preds_num += len(true_word)\n","    bad_preds += bad_chars\n","    \n","  return (good_preds_num / all_preds_num), bad_preds\n","\n","\n","# all_pred_words [num_samples, ?] - сампли з передбаченими словами\n","# all_true_words [num_samples, word_lens] - сампли з правдивими словами\n","# word_mode - якщо 1, то точність за словами 0 - за символами \n","#\n","# accuraccy - точність\n","def get_accuraccy(all_pred_words, all_true_words, word_mode=0):\n","  if word_mode:\n","    accuraccy, bad_preds = get_word_accuraccy(all_pred_words, all_true_words)\n","  else:\n","    accuraccy, bad_preds = get_char_accuraccy(all_pred_words, all_true_words)\n","  return accuraccy, bad_preds\n","  \n","\n","# data_gen - генератор даних\n","# num_samples - кількість слів (число кратне batch_size)\n","# alphabet_str[num_classes] - алфавіт (lower+upper+digits+.) \n","# data_gen_kwargs:  \n","# font_names - список назв шрифтів\n","# batch_size - розмір батчу\n","# max_word_len - максимальна довжина слова\n","# font_p - ймовірності кожного шрифта (не обовяково)\n","# data_dir_path  - адреса директорії з даними\n","# \n","# all_pred_words [num_samples, ?] - сампли з передбаченими словами\n","# all_true_words [num_samples, word_lens] - сампли з правдивими словами\n","def get_all_predictions2(data_gen, num_samples, alphabet_str, **data_gen_kwargs):\n","  all_pred_words = []\n","  all_true_words = []\n","  for i, data in enumerate(data_gen(**data_gen_kwargs)):\n","    inputs, outputs = data\n","    batch_x, batch_y, word_lens = inputs['the_input'], inputs['the_labels'], inputs['label_length']\n","    y_pred = model.predict_on_batch(batch_x)\n","    pred_words, true_words = get_words_from_y(y_pred, batch_y, word_lens, alphabet_str)\n","    all_pred_words += pred_words\n","    all_true_words += true_words\n","    \n","    if i >= int(num_samples/data_gen_kwargs['batch_size'])-1:\n","      break\n","  return all_pred_words, all_true_words\n","\n","# перевірити чи не переплутав // і %\n","# додати паддінг до вирізаних із зображення слів\n","# видалити старі розміром 96 на 24\n","# зробити хелв неуе new додавши z q j 0-9 ...\n","# затрейнити на ній робото\n","# зробити норм тест з цифрами і буквами\n","# затестити на зображенні\n","# підсвітка\n","# "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWmAsQ5IWzjN","colab_type":"text"},"source":["## get preds and acc"]},{"cell_type":"markdown","metadata":{"id":"Qpuz_xFl2g2h","colab_type":"text"},"source":["\n","\n"," > **all predictions, accuraccy**\n","\n"]},{"cell_type":"code","metadata":{"id":"d_KOppss0lMX","colab_type":"code","outputId":"783771ae-1398-422c-964e-7c4b70a736be","executionInfo":{"status":"ok","timestamp":1567679123647,"user_tz":-180,"elapsed":21010,"user":{"displayName":"yura it's raining today","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAhDS418TZysN6H8syy27wT4d5PQoynXsSD92Xk=s64","userId":"15239198560947031173"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["%%time\n","train_dir_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/train/x_new'\n","\n","test_dir_path = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test/x' # chars(-z,-j,-q) + .\n","test_dir_path3 = r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test/x_new'\n","\n","data_gen_kwargs1 = {\n","  'font_names' : ['roboto_mono'], #[i for i in font_paths.keys() if i not in late_paths],\n","  'batch_size' : 64,\n","  'max_word_len' : 16,\n","  'font_p' : None,\n","  'data_dir_path' : test_dir_path\n","}\n","\n","data_gen_kwargs = {\n","   'batch_size' : 80,\n","  'font_name' : 'roboto_mono',\n","  'max_word_len' : 16,\n","  'alphabet_arr': list(string.ascii_letters + string.digits + '.'),\n","  'font_paths': font_paths\n","}\n","\n","\n","num_samples = 128\n","alphabet_str = string.ascii_letters + string.digits + '.'\n","\n","all_pred_words, all_true_words = get_all_predictions2(data_gen2, num_samples, alphabet_str, **data_gen_kwargs1)\n","accuraccy, bad_preds = get_accuraccy(all_pred_words, all_true_words, word_mode=0)\n","\n","print('p: {}'.format(all_pred_words))\n","print('t: {}'.format(all_true_words))\n","print('acc: ', accuraccy)\n","print('bad_p: {}'.format(bad_preds))"],"execution_count":125,"outputs":[{"output_type":"stream","text":["p: ['style', 'sell', 'Aelenshalo', 'Taz4', 'salisitude', 'sakKreaAed', 'SiXAing', 'talKeS', 'famm', 'ouIger', 'ta', 'nar', 'nee6', 'rekk', 'skug', 'fakd', 'drew', 'Cz', 'na', 'hukteS', 'Taz4', 'severel', 'travelling', 'Wzvde', 'tJekSo', 'Cz', 'sao', 'Brep', 'skug', 'ikquimo', 'dispavered', 'hukteS', 'Wzvde', 'Wzvde', 'sao', 'enzugh', 'zne', 'Made', 'AetL', 'prsCesAing', 'led', 'SentimenAe', 'nee6', 'famm', 'eetw', 'severel', 'ell', 'tzreL', 'sakSreaAedw', 'Brep', 'liAiSe', 'Yaq', 'led', 'bed', 'hexrAed', 'nar', 'Xindnese', 'ye', 'SAristlo', 'Taz4', 'zn', 'lengAh', 'rekk', 'travelling', 'If', 'far', 'rekev4', 'rekev4', 'Brep', 'entreaAiee', 'veL', 'entire', 'suferL', 'suferL', 'ExpVessisn', 'insVesing', 'hukteS', 'in', 'here', 'far', 'Zaumee', 'saw', 'far', 'ds', 'Zaumee', 'zkeL', 'ds', 'saw', 'beL', 'his', 'es', 'suferL', 'rekS', 'beL', 'eetimaAing', 'es', 'ds', 'vavSRo', 'pesple', 'gone', 'Zaumee', 'beL', 'Aekw', 'salisitudeL', 'elAeretisn', 'far', 'Brep', 'Able', 'Aekw', 'apepAence', 'enzugh', 'suferL', 'add', 'gone', 'eir', 'vavSRo', 'pesple', 'expressisn', 'thiKge', 'entire', 'sao', 'zn', 'Rein', 'vaqnz', 'leter', 'eetw', 'daL', 'hukteS']\n","t: ['style', 'call', 'melancholy', 'Took', 'solicitude', 'contrasted', 'Sitting', 'talked', 'form', 'vulgar', 'to', 'nor', 'neat', 'rank', 'snug', 'fond', 'draw', 'it', 'no', 'hunted', 'Took', 'several', 'travelling', 'Words', 'twenty', 'it', 'say', 'Draw', 'snug', 'inquiry', 'discovered', 'hunted', 'Words', 'Words', 'say', 'enough', 'one', 'Made', 'met.', 'projecting', 'led', 'Sentiments', 'neat', 'form', 'eat.', 'several', 'all', 'tore.', 'contrasted.', 'Draw', 'Limits', 'You', 'led', 'bed', 'hearted', 'nor', 'kindness', 'ye', 'Strictly', 'Took', 'on', 'length', 'rank', 'travelling', 'If', 'for', 'remark', 'remark', 'Draw', 'entreaties', 'we.', 'entire', 'suffer.', 'suffer.', 'Expression', 'increasing', 'hunted', 'in', 'here', 'for', 'Course', 'now', 'for', 'do', 'Course', 'one.', 'do', 'now', 'be.', 'his', 'as', 'suffer.', 'rent', 'be.', 'estimating', 'as', 'do', 'worthy', 'people', 'gone', 'Course', 'be.', 'man.', 'solicitude.', 'alteration', 'for', 'Draw', 'Able', 'man.', 'acceptance', 'enough', 'suffer.', 'add', 'gone', 'sir', 'worthy', 'people', 'expression', 'things', 'entire', 'say', 'on', 'Pain', 'wound', 'Latter', 'eat.', 'do.', 'hunted']\n","acc:  0.5874233128834356\n","bad_p: ['s', 'e', 'A', 'e', 's', 'a', 'o', 'a', 'z', '4', 'a', 's', 's', 'a', 'k', 'K', 'e', 'a', 'A', 'X', 'A', 'K', 'S', 'a', 'm', 'o', 'I', 'e', 'a', 'a', 'e', '6', 'e', 'k', 'k', 'a', 'k', 'e', 'C', 'z', 'a', 'k', 'S', 'a', 'z', '4', 'e', 'z', 'v', 'e', 'J', 'k', 'S', 'o', 'C', 'z', 'o', 'B', 'e', 'p', 'k', 'k', 'm', 'o', 'p', 'a', 'k', 'S', 'z', 'v', 'e', 'z', 'v', 'e', 'o', 'z', 'z', 'A', 'L', 's', 'C', 's', 'A', 'A', 'e', 'e', '6', 'a', 'm', 'e', 'w', 'e', 'e', 'z', 'L', 's', 'a', 'k', 'S', 'e', 'a', 'A', 'w', 'B', 'e', 'p', 'l', 'A', 'S', 'e', 'a', 'q', 'x', 'A', 'a', 'X', 'e', 'A', 's', 'o', 'a', 'z', '4', 'z', 'A', 'e', 'k', 'a', 'k', 'e', 'v', '4', 'k', 'e', 'v', '4', 'B', 'e', 'p', 'A', 'e', 'v', 'L', 'e', 'r', 'L', 'e', 'r', 'L', 'V', 's', 's', 'V', 's', 'i', 'n', 'g', 'k', 'S', 'a', 'Z', 'a', 'm', 'e', 's', 'a', 'a', 's', 'Z', 'a', 'm', 'e', 'z', 'k', 'L', 's', 's', 'a', 'L', 'e', 'e', 'r', 'L', 'k', 'S', 'L', 'e', 'A', 'e', 's', 'v', 'a', 'v', 'S', 'R', 'o', 's', 'Z', 'a', 'm', 'e', 'L', 'A', 'e', 'k', 'w', 'a', 's', 'L', 'e', 'A', 'e', 's', 'a', 'B', 'e', 'p', 'A', 'e', 'k', 'w', 'p', 'e', 'p', 'A', 'e', 'n', 'c', 'e', 'z', 'e', 'r', 'L', 'e', 'v', 'a', 'v', 'S', 'R', 'o', 's', 's', 'K', 'e', 'o', 'z', 'R', 'e', 'v', 'a', 'q', 'z', 'l', 'e', 'e', 'r', 'e', 'w', 'a', 'L', 'k', 'S']\n","CPU times: user 420 ms, sys: 102 ms, total: 523 ms\n","Wall time: 19.8 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0e8PxAYoPXzj","colab_type":"code","colab":{}},"source":["from collections import Counter\n","\n","cnt = Counter(bad_preds)\n","print(cnt)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xLGXzyAJTkH","colab_type":"text"},"source":["# autocorrect"]},{"cell_type":"code","metadata":{"id":"y8HLnT1mJXBV","colab_type":"code","colab":{}},"source":["! pip install textblob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbQveDKNJXsK","colab_type":"code","colab":{}},"source":["from textblob import TextBlob\n","\n","\n","def autocorrect(all_pred_words):\n","  text = ' '.join(all_pred_words)\n","  b = TextBlob(text)\n","  all_pred_words_corrected = b.correct().split(' ')\n","  return all_pred_words_corrected\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"opV_1_fHOsid","colab_type":"code","colab":{}},"source":["\n","accuraccy, bad_preds = get_accuraccy(all_pred_words, all_true_words, word_mode=0)\n","all_pred_words_corrected =  autocorrect(all_pred_words)\n","\n","print('p: {}'.format(all_pred_words))\n","print('c: {}'.format(all_pred_words_corrected))\n","print('t: {}'.format(all_true_words))\n","\n","after_accuraccy, bad_preds = get_accuraccy(all_pred_words_corrected, all_true_words, word_mode=0)\n","\n","print('accuraccy:', accuraccy, sep =' ')\n","print('after correct accuraccy:', after_accuraccy, sep =' ')\n","loss_fall = 1 - (1-after_accuraccy) / (1-accuraccy) # який відсоток займає after correct помилка від простої\n","print('loss fall: {} %'.format(loss_fall*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W15hLmbt7XQr","colab_type":"text"},"source":["# highlight"]},{"cell_type":"markdown","metadata":{"id":"OqUT-NA178g6","colab_type":"text"},"source":["## get word coords"]},{"cell_type":"code","metadata":{"id":"lZpJ4ANC76G_","colab_type":"code","colab":{}},"source":["def get_words(img):\n","    \"\"\"\n","    Function returns bboxes for each word and for chars in it.\n","    \n","    Parameters\n","    ----------\n","    img: np.array\n","    Thresholded grayscale image.\n","    \n","    Returns\n","    -------\n","    words: list\n","    Array of words bboxes in the form (x, y, w, h) where (x, y) - lower left point, h - height of bbox, w - width\n","    \n","    chars: list of lists\n","    Array which contains arrays that represent coordinates of each  character in word\n","    \"\"\"\n","    \n","    # here we take maximum elements by word so we can detect rows with words\n","    # if there is a word the value would be 255, if not - 0\n","    maxs = np.max(img, axis=1) # maximum element by row\n","    indxs_y = np.where(maxs[:-1] != maxs[1:])[0] # indexes of elements where is transition from 255 to 0 or vice versa\n","    words = [] # list for storing words bboxes\n","    chars = [] # list for storing characters bboxes\n","\n","    # In this loop we do the same operation but get max values by column so we can know\n","    # Where the character ends and starts\n","    for indx_y in range(0, len(indxs_y)-1, 2):\n","\n","        distances = [] # list to store distances between \n","        cordinates = []\n","        row = img[indxs_y[indx_y]:indxs_y[indx_y+1]] # take row of the text\n","        maxs1 = np.max(row, axis=0)\n","        indxs_x = np.where(maxs1[:-1] != maxs1[1:])[0]\n","        last_x = indxs_x[0]\n","        for indx_x in range(0, len(indxs_x)-1, 2):\n","            cordinates.append((\n","                indxs_x[indx_x],\n","                indxs_y[indx_y],\n","                indxs_x[indx_x+1] - indxs_x[indx_x],\n","                indxs_y[indx_y+1] - indxs_y[indx_y]\n","            ))\n","            distances.append(indxs_x[indx_x] - last_x)\n","            last_x = indxs_x[indx_x+1]\n","\n","        temp = get_words_cordinates(cordinates, distances)\n","        if temp == -1:\n","            continue\n","        \n","        temp_words = temp[0]\n","        temp_chars = temp[1]\n","            \n","        words += temp_words\n","        chars += temp_chars\n","        \n","    return words, chars\n","\n","\n","def get_words_cordinates(cordinates, distances):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    cordinates: tuple or list which contains bboxes for each character in form - (x, y, w, h), where (x, y) - lower left point, h -       height of bbox, w - width\n","    \n","    \"\"\"\n","    hist, bins = np.histogram(distances) # get histogram of distances between characters\n","    \n","    divider = find_divider(hist, bins) # get value by which we will decide if character belong to current word or starts next\n","    words = []\n","    chars = []\n","    x, y, w, h = cordinates[0]\n","    \n","    if w*h < 25: # weeding out anomalies\n","        return -1\n","    \n","    if check_for_one_word_in_line(distances): # check is it only one word in row so we can treat this case different\n","        for i, (x_t, y_t, w_t, h_t) in enumerate(cordinates[1:]):\n","            y = min(y, y_t)\n","            h = max(h, h_t)\n","            w += w_t + distances[i]\n","        \n","        words.append((x, y, w+distances[i+1], h))\n","        chars = cordinates\n","    else:\n","        start = 0 # start index for current word\n","        end = 0 # end index for current word\n","        for i, item in enumerate(distances[1:]):\n","            if item < divider:\n","                x_t, y_t, w_t, h_t = cordinates[i+1]\n","                y = min(y, y_t)\n","                w += w_t + item\n","                h = max(h, h_t)\n","                end += 1\n","            else:\n","                words.append((x, y, w, h))\n","                x, y, w, h = cordinates[i+1]\n","                end += 1\n","                chars.append(cordinates[start:end])\n","                start = end\n","\n","        chars.append(cordinates[start:])\n","        words.append((x, y, w, h))\n","        \n","    return words, chars\n","\n","def find_divider(hist, bins):\n","    \"\"\"\n","    Function finds distance value.\n","    \n","    If distance between separate characters more than this value, characters are in the same word and if distance lower -                 characters belong to different words\n","    \"\"\"\n","    left = 0\n","    right = len(hist)-1\n","    \n","    while hist[left] == 0:\n","        left += 1\n","        \n","    while hist[right] == 0:\n","        right -= 1\n","    \n","    divider = bins[left]*0.5 + bins[right]*0.5\n","    \n","    return divider\n","        \n","def check_for_one_word_in_line(distances):\n","    \"\"\"\n","    Function chechs whether it is only one word in line\n","    \"\"\"\n","    mean = np.mean(distances)\n","    var = np.var(distances)\n","    \n","    if mean > var:\n","        return True\n","\n","    return False       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwJs6JLE7qp_","colab_type":"text"},"source":["## get words images by coords"]},{"cell_type":"code","metadata":{"id":"I4wpcyJr7n2q","colab_type":"code","colab":{}},"source":["# image_address - адреса зображення\n","# words_images - зображення слів із одного зображення тексту \n","def get_words_images_and_cords(image_address, save_words_bboxes=0, save_dir_path=None):\n","  image = cv2.imread(image_address, 0)\n","  ret, thresh = cv2.threshold(image,180,255,cv2.THRESH_BINARY_INV)\n","  #w,h = image.shape[1], image.shape[0]\n","  #image = cv2.resize(image, (w*2, h*2), interpolation = cv2.INTER_CUBIC)\n","  word_coords, _ = get_words(thresh)\n","  if save_words_bboxes:\n","    plot_bboxes(word_coords, deepcopy(image), dir_path=save_dir_path)\n","  word_images = get_images_by_coords_and_preprocess(word_coords, image)                            # !!! PREPROCESS\n","  return word_images, word_coords, image\n","  \n","\n","# images_dir_path - адреса директорії зі зображеннями\n","# all_words_images [images, word_images] - зображення всіх слів із кожного зображення тексту \n","def get_all_words_images_and_coords(images_dir_path, write=0, save_dir_path=None):\n","  all_words_images = []\n","  all_words_coords = []\n","  all_text_images = []\n","  for address in sort_int_filenames(os.listdir(images_dir_path)):                                                     \n","    if address.split('.')[-1] == 'jpg' or 'JPG':\n","      word_images, word_coords, text_image = get_words_images_and_cords(os.path.join(images_dir_path, address), write, save_dir_path)\n","      all_words_images.append(word_images)\n","      all_words_coords.append(word_coords)\n","      all_text_images.append(text_image)\n","  return all_words_images, all_words_coords, all_text_images\n","\n","\n","# filenames - масив назв файлів: ['2.jpg', '1.jpg']\n","# sorted_filenames - посортований масив назв файлів: ['1.jpg', '2.jpg']\n","def sort_int_filenames(filenames):\n","  names = [int(name.split('.')[0]) for name in filenames]\n","  sorted_names = sorted(names)\n","  sorted_filenames = [str(name)+'.JPG' for name in sorted_names]\n","  return sorted_filenames\n","\n","\n","# бере частини із зображення згідно координатам bbox\n","def get_images_by_coords_and_preprocess(coords, image):\n","  images = []\n","  for (x,y,w,h) in coords:\n","    word_image = deepcopy(image[y:y+h, x:x+w])\n","    word_image = pad_image(word_image)\n","    word_image = cv2.bilateralFilter(word_image,9,100,100) #diameter,\n","    images.append(word_image)\n","  return images\n","\n","\n","# виводить зображення в заданому colorspace\n","def plot_image(image, colorspace='gray'):\n","  if colorspace:\n","    plt.imshow(image, colorspace) \n","  plt.show()\n","  \n","\n","# image [h, w] - зображення\n","#\n","# pad_image [32, 128] - змінене зображення \n","def pad_image(image):\n","  h, w = image.shape\n","  if w>128:\n","    image = cv2.resize(image, (128, int((128/w)*h)), interpolation=cv2.INTER_AREA)\n","  elif h>32:\n","    image = cv2.resize(image, (int((32/h)*w), 32), interpolation=cv2.INTER_AREA)\n","  h, w = image.shape\n","\n","  pad_h = 32-h\n","  pad_w = 128-w\n","  pad_image = np.pad(image, ((pad_h, 0), (0,pad_w)), 'constant', constant_values=(255))\n","  return pad_image\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJ72uiNu7vpt","colab_type":"text"},"source":["## highlight"]},{"cell_type":"code","metadata":{"id":"YIWj6UKh7a1Q","colab_type":"code","colab":{}},"source":["from collections import Counter\n","# y_pred [batch, 32, num_classes] - батч розподілів ймовірностей \n","# alphabet_str [num_classes] - алфавіт, символи всіх класів (lower+upper+digits+.)\n","#\n","# pred_words [batch_size, ?] - батч передбачених слів\n","def get_words_arr_from_y(y_pred, alphabet_str):\n","  pred_words = []\n","  for i in range(len(y_pred)):\n","    pred_word = decode_label(y_pred[i], alphabet_str)\n","    pred_words.append(pred_word)\n","    \n","  return pred_words\n","\n","\n","# image [32, 128] - зображення тексту\n","# words [words_num] - масив передбачених слів\n","# words_coords [words_num] - масив передбачених координат\n","def highlight_most_frequent(image, words, words_coords):\n","  highlighted_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n","  words = [w.lower() for w in words]\n","  counter = Counter(words)\n","  if not counter.most_common(1):\n","    return\n","  most_frequent_word = counter.most_common(1)[0][0]\n","  for i, word in enumerate(words):\n","    if word == most_frequent_word:\n","      (x, y, w, h) = words_coords[i]\n","      temp_area = deepcopy(highlighted_img[y:y+h, x:x+w])\n","      ret, thresh = cv2.threshold(image,127,255,cv2.THRESH_BINARY_INV)\n","      mask = deepcopy(thresh[y:y+h, x:x+w])\n","      temp_area[mask == 255] = [255, 0, 0]\n","      highlighted_img[y:y+h, x:x+w] = temp_area \n","  return highlighted_img\n","\n","\n","# font_names [fonts_n] - масив назв шрифтів\n","# imgs_range [2] - індекси початкового та кіневого-1 зображення \n","# alphabet_str - стрічка з алфавітом\n","# labels_path - шлях до y.txt\n","# save_dir_path - шлях до директорії для зображеннь\n","def save_highlights_from_images(model, images_dir_path, labels_path, batch_size, alphabet_str, save_dir_path):\n","  print('get_words')\n","  all_words_images, all_words_coords, all_text_images = get_all_words_images_and_coords(images_dir_path, write=0, save_dir_path=None)\n","  \n","  print('get_labels and indexes')\n","  #all_labels_words = get_labels_words(labels_path)\n","  #all_labels_indexes = get_labels_indexes(all_labels_words, alphabet_str) #[img_n, word_n]\n","  \n","  print('start get preds batches')\n","  for i in range(len(all_words_images)):\n","    img_pred_words = []\n","    offset = 0\n","    word_num = len(all_words_images[i])\n","    \n","    while offset < word_num:\n","      batch_x = []\n","      if offset+batch_size > word_num:\n","        batch_x += [all_words_images[i][offset:]]\n","        offset = word_num\n","      else:\n","        batch_x += [all_words_images[i][offset:batch_size]]\n","        offset+=batch_size\n","        \n","      batch_x = np.asarray(batch_x).T  \n","      batch_x = np.reshape(batch_x, [-1,128,32,1])\n","      y_pred = model.predict_on_batch(batch_x)  \n","      \n","      img_pred_words += get_words_arr_from_y(y_pred, alphabet_str)\n","      \n","    print('highlight')\n","    highlighted_image = highlight_most_frequent(all_text_images[i], img_pred_words, all_words_coords[i]) \n","    print('save')\n","    cv2.imwrite(os.path.join(save_dir_path, str(i)+'.jpg'), highlighted_image)\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQd6NIZ38QY9","colab_type":"text"},"source":["## test"]},{"cell_type":"code","metadata":{"id":"vVj5yEjc7dpE","colab_type":"code","colab":{}},"source":["%%time\n","kwargs = {\n","    'model': model,\n","    'images_dir_path': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test_on_text/text_imgs_20',\n","    'labels_path': r'',\n","    'batch_size': 5,\n","    'alphabet_str': string.ascii_letters + string.digits + '.',\n","    'save_dir_path': r'/content/drive/My Drive/Colab Notebooks/course_project/data_crnn/test_on_text/result'\n","}\n","\n","save_highlights_from_images(**kwargs)"],"execution_count":0,"outputs":[]}]}